<script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>
<script type="module">
  panViewer('#cat-pikachiu');
  panViewer('#shiba-haru');
  function panViewer(modelid) {
    const modelViewer = document.querySelector(modelid);
    modelViewer.addEventListener('load', () => {
        for (const material of modelViewer.model.materials) {
          // Removes occlusion map from all materials.
          material.occlusionTexture.setTexture(null);
        }
      });
    
    // change color
    const subid = '#color-controls-' + modelid.split("#")[1];
    console.log('Changing color to: ', subid);
    document.querySelector(subid).addEventListener('click', (event) => {
      const colorString = event.target.dataset.color;
      if (!colorString) {
        return;
      }
      const modelSrc = modelViewer.getAttribute('src');
      //debugger;
      if (colorString == "0") {
          const submodelSrc = modelSrc.split(".glb")[0];
          modelViewer.setAttribute('src', submodelSrc + '-gray.glb');
          event.target.dataset.color = "1";
      } else if (colorString == "1") {
          const submodelSrc = modelSrc.split("-gray.glb")[0];
          modelViewer.setAttribute('src', submodelSrc + '.glb');
          event.target.dataset.color = "0";
      }
    });
  }
</script>

<html xmlns="http://www.w3.org/1999/xhtml"><head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="robots" content="noindex">
    <link rel="StyleSheet" href="./files/style.css" type="text/css" media="all">

    <title>Uni3D: Single Video-based Dynamic 3D Modeling for Humans and Animals with a Unified Template
      </title>

    <style type="text/css">
      body {
	    font-family : Times;
	    background-color : #f2f2f2;
	    font-size : 15px;
      }

      .content {
	    width : 800px;
	    padding : 25px 25px;
	    margin : 25px auto;
	    background-color : #fff;
	    border-radius: 20px;
      }
      .description {
        font-family: "Times";
        white-space: pre;
        text-align: left;
      }

      .content-title {
	    background-color : inherit;
	    margin-bottom : 0;
	    padding-bottom : 0;
      }

      a, a:visited {
	    text-decoration: none;
	    color : blue;
      }

      .anchor {
      color: inherit;
      }
      #authors {
	    text-align : center;
      }

      #conference {
	    text-align : center;
	    font-style : italic;
      }

      #authors a {
	    margin : 0 10px;
      }

      h1 {
	    text-align : center;
	    font-family : Times;
	    font-size : 35px;
      }

      h2 {
	    font-family : Times;
	    font-size : 25px;
	    padding : 0; margin : 10px;
      }

      h3 {
	    font-family : Times;
	    font-size : 20px;
	    padding : 0; margin : 10px;
      }

      p {
	    font-family : Times;
	    line-height : 130%;
	    margin : 10px;
      }

      big {
	    font-family : Times;
	    font-size : 20px;
      }

      li {
	    margin : 10px 0;
      }

      .samples {
	    float : left;
	    width : 50%;
	    text-align : center;
      }

      .cond {
	    float : left;
	    margin : 0 40px;
      }

      .cond-container {
	    width : 700px;
	    margin : 0 auto;
	    text-align : center;
      }
     #vidalign {
         display: block;
         margin: 0px;
         padding: 0px;
         position: relative;
         top: 90px;
         height: auto;
         max-width: auto;
         overflow-y: hidden;
         overflow-x:auto;
         word-wrap:normal;
         white-space:nowrap;
     }

    /* Add a black background color to the top navigation */
    .topnav {
      background-color: rgba(0,0,0,0.2);
      z-index:1; 
      overflow: hidden;
      position: fixed;
      top: 0; /* Position the navbar at the top of the page */
      width: 100%; /* Full width */
    }
    
    /* Style the links inside the navigation bar */
    .topnav a {
      float: left;
      color: #333;
      text-align: center;
      padding: 14px 16px;
      text-decoration: none;
      font-size: 17px;
    }
    
    /* Change the color of links on hover */
    .topnav a:hover {
      background-color: #ddd;
      color: black;
    }
    
    /* Add a color to the active/current link */
    .topnav a.active {
      background-color: #04AA6D;
      color: white;
    }

    </style>

    <style>
    model-viewer {
      width: 300px;
      height: 300px;
    }
    </style>

  </head>

    <div class="topnav">
      <a class="active" href="#top">Top</a>
      <a href="#abs">Paper/Code</a>
<!--       <a href="#vid">Video</a> -->
      <a href="#results">Results</a>
<!--       <a href="#aba">Ablations</a> -->
<!--       <a href="#bib">Bibtex</a> -->
    </div>

    <div id="top" class="content content-title" style="text-align: center;">
	    <h1>Uni3D: Single Video-based Dynamic 3D Modeling for Humans and Animals with a Unified Template </h1>
<!-- 	<big style="color:grey;"> CVPR 2022 - Oral Presentation </big>     -->
	<p id="authors">
      <table align="center" style="width:80%; text-align:center; table-layout: fixed">
        <tr>
	      <th><a href="https://xiangyueliu.github.io/">Xiangyue Liu<sup>1</sup></a></th>
	      <th><a href="https://ericyi.github.io/">Li Yi<sup>1</sup></a></th>
	      <th>Weixin Xu<sup>2</sup></a></th>
<!-- 	     <th><a href="https://zsc.github.io/">Shuchang Zhou<sup>2</sup></a></th>
	      <th><a href="http://people.iiis.tsinghua.edu.cn/~gaoyang/yang-gao.weebly.com/index.html">Yang Gao<sup>1</sup></a></th> -->
        </tr>
      </table>
      <table align="center" style="width:60%; text-align:center; table-layout: fixed">
        <tr>
	     <th><a href="https://zsc.github.io/">Shuchang Zhou<sup>2</sup></a></th>
	      <th><a href="http://people.iiis.tsinghua.edu.cn/~gaoyang/yang-gao.weebly.com/index.html">Yang Gao<sup>1</sup></a></th>
        </tr>
      </table>
      <table align="center" style="width:60%; text-align:center; table-layout: fixed">
        <th><sup>1</sup>Tsinghua University</th>
        <th><sup>2</sup>MEGVII Research</th>
      </table>
	    </p>
	    <p>
	    </p>
    </div>



    <div class="content">
      <figure style="font-family: Times; font-weight: normal; margin: 0px; padding: 0px; border: 0px; text-align: left">
         <p style="text-align:center;">
         <img src="./vids/fig1.jpg" width="750">
         </p>
         <br>
	      <figcaption> <b>Uni3D</b> robustly reconstructs dynamic 3D models of humans and animals from single short casual video frames.
              <b>Left</b>: Input an individual video of an object. <b>Middle</b>: A unified coarse shape, 3D shape in canonical space with skinning weight as surface color.
              <b>Right</b>: Unconstrained posed reconstruction at each time instance and different views.
      </figure>
      <br>
<!--       <figure style="font-family: Times; font-weight: normal; margin: 0px; padding: 0px; border: 0px; text-align: left">
         <p style="text-align:center;">
         <img src="./vids/teaser-small.gif" width="750">
         <p>
	      <figcaption> 
              <b>Left</b>: Input videos; 
              <b>Right</b>: Reconstruction at each time instance. Correspondences are shown as the same colors. -->
    </div>
    
    <div id="abs" class="content">
      <h2>Abstract</h2>
      <p>
      Prior work for recovering realistic 3D models for humans and animals from a single video needs either a category-specific shape template or initializing shape as a unit sphere. 
	      Such methods do not applicable to diverse kinds of categories or can not handle fast object motions well. Our method, called Uni3D, goes beyond current work in several important ways. 
	      First, we introduce a unified shape to be more appropriate for representing humans and animals. 
	      But, even with a better shape model, the problem of reconstructing dynamic models from a single video is still challenging because fast object motions commonly exist in casually captured videos. 
	      To address this, we propose a novel feature blend skinning deformation model that leverages long-range dense correspondence feature matching information. 
	      Evaluations on real and synthetic datasets validate that Uni3D achieves state-of-the-art 3D shape reconstruction performance regarding both geometry and texture quality. 
	      Notably, Uni3D takes a step toward general high-fidelity human and animal model reconstruction from a single casual video. 
	    </p>
      <div id="teaser" style="margin: 12px; text-align: left;border-top: 1px solid lightgray;padding-top: 12px;">
<!--           <a href="https://github.com/facebookresearch/banmo"> -->
	        <strong>[Code]</strong>
          </a>
<!--           <a href="./banmo-cvpr.pdf"> -->
	        <strong>[Paper]</strong>
          </a>

          <!--
          <a href="https://arxiv.org/abs/2112.12761">
	        <strong>[Arxiv]</strong>
	      </a> 
          -->
	      
      </div>
    </div>
   
<!--  <div id="vid" class="content">
      <div style="float: right; width:70px; margin-top: 0px; margin-bottom: 25px">
      </div>
      <h2>Video
      </h2>
         <iframe width="810" height="520" src="https://www.youtube.com/embed/1NUa-yvFGA0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      <div id="teaser" style="margin: 12px; text-align: left;border-top: 1px solid lightgray;padding-top: 12px;">
          <a href='./banmo-video-cvpr.mp4'> 
              <strong>[Download] </strong>
          </a>
      </div>
    </div>  -->
   
    <div id="results" class="content">
      <h2>Results</h2>

<!--     <h3>Casual-cat (11 videos) <a href='./cats.html'> >> [Click for more]</a></h3> -->
    <h3>Dog-pidun (7s)</h3>
	<br>
	<center>
		<table align=center width=810px>
			<tr>
				<td width=810px>
                         <video playsinline controls autoplay muted loop width="100%">
                          <source  src="./vids/cat-pikachiu-{0}-all.mp4" type="video/mp4">
                         </video>
				</td>
			</tr>
		</table>
		<table align=center width=810px>
			<tr>
				<td>
				 Casual-cat-0. Top left: reference image overlayed with input densepose features. Top middle: reconstructed  1st frame shape. Top right: recovered articulations in the canoincal space. Bottom row: reconstruction from front/side/top viewpoints. Correspondences are shown as the same color.
				</td>
			</tr>
		</table>
	</center>
	  
	    
    <h3>Human-cap (9s)</h3>
	<br>
	<center>
<!-- 		<table align=center width=810px> -->
		<table align=center width=610px>
			<tr>
				<td width=810px>
                         <video playsinline controls autoplay muted loop width="100%">
                          <source  src="./vids/human-dapose-single-{0}-all-three.mp4" type="video/mp4">
                         </video>
				</td>
			</tr>
		</table>
		<table align=center width=810px>
			<tr>
				<td>
				 Casual-cat-0. Top left: reference image overlayed with input densepose features. Top middle: reconstructed  1st frame shape. Top right: recovered articulations in the canoincal space. Bottom row: reconstruction from front/side/top viewpoints. Correspondences are shown as the same color.
				</td>
			</tr>
		</table>
	</center>
	    
	    
    <h3>Swan-snow (10s)</h3>
	<br>
	<center>
		<table align=center width=810px>
			<tr>
				<td width=810px>
                         <video playsinline controls autoplay muted loop width="100%">
                          <source  src="./vids/bird1-{0}-all-three.mp4" type="video/mp4">
                         </video>
				</td>
			</tr>
		</table>
		<table align=center width=810px>
			<tr>
				<td>
				 Casual-cat-0. Top left: reference image overlayed with input densepose features. Top middle: reconstructed  1st frame shape. Top right: recovered articulations in the canoincal space. Bottom row: reconstruction from front/side/top viewpoints. Correspondences are shown as the same color.
				</td>
			</tr>
		</table>
	</center>
	    
	    
	    
<!--     <div>
        <h3>Articulated shape:</h3>
	<center>
    <div class="mesh">
    <model-viewer autoplay ar shadow-intensity="1"  id="cat-pikachiu" src="./meshes/cat-pikachiu.glb" alt="Articulated shape" ar ar-modes="webxr scene-viewer quick-look" environment-image="neutral" auto-rotate camera-controls>
       <div class="controls" ,="" id="color-controls-cat-pikachiu">
        <button data-color="0">Color on/off</button>
       </div>
    </model-viewer>
    </div>
	</center>
    </div> -->
	    
<!-- 	<hr>
    <br> -->


<!--     <h3>Human-cap (9s)</h3>
    <br>
	<center>
		<table align=center width=810px>
			<tr>
				<td width=810px>
                         <video playsinline controls autoplay muted loop width="100%">
                             <source  src="./vids/human-dapose-single-{0}-all-three.mp4" type="video/mp4">
                         </video>
				</td>
			</tr>
		</table>
		<table align=center width=810px>
			<tr>
				<td>
				 Dog(Shiba)-Haru-5. Top left: reference image overlayed with input densepose features. Top middle: reconstructed  1st frame shape. Top right: recovered articulations in the canoincal space. Bottom row: reconstruction from front/side/top viewpoints. Correspondences are shown as the same color.
				</td>
		</table>
	</center>
    <div>
        <h3>Articulated shape:</h3>
	<center>
    <div class="mesh">
    <model-viewer autoplay ar shadow-intensity="1"  id="shiba-haru" src="./meshes/shiba-haru.glb" alt="Articulated shape" ar ar-modes="webxr scene-viewer quick-look" environment-image="neutral" auto-rotate camera-controls>
       <div class="controls" ,="" id="color-controls-shiba-haru">
        <button data-color="0">Color on/off</button>
       </div>
    </model-viewer>
    </div>
	</center>
      <div id="teaser" style="margin: 12px; text-align: left;border-top: 1px solid lightgray;padding-top: 12px;">
      <h3>More results</h3>
      <p>
          <a href='./results.html#cmp'> <strong> [Comparison] </strong></a>
          <a href='./results.html#psn1'> <strong> [Human-cap] </strong></a>
          <a href='./results.html#cat2'> <strong> [Cat-Coco] </strong></a>
          <a href='./results.html#pgn'> <strong> [Penguins] </strong></a>
          <a href='./results.html#dog2'> <strong> [Dog-Tetres] </strong></a>
          <a href='./results.html#rbt'> <strong> [Robot-Laikago] </strong></a>
          <a href='./results.html#psn2'> <strong> [Dancer-AMA] </strong></a>
          <a href='./results.html#egl'> <strong> [Eagle] </strong></a>
          <a href='./results.html#hnd'> <strong> [Hands] </strong></a>
      </p>
      </div>
    </div>
    </div>  -->


   
<!--     <div id="bib" class="content">
            <h2>Bibtex</h2>
            <p class="description">@inproceedings{yang2022banmo,
  title={BANMo: Building Animatable 3D Neural Models from Many Casual Videos},
  author={Yang, Gengshan 
      and Vo, Minh
      and Neverova, Natalia
      and Ramanan, Deva
      and Vedaldi, Andrea
      and Joo, Hanbyul},
  booktitle = {CVPR},
  year={2022}
}  </p>
    </div> -->
    


<!--     <div class="content">
            <h2>Related projects</h2>
	    <p>
        Deformable shape reconstruction from video(s):<br>
	    <a href="https://viser-shape.github.io/"> ViSER: Video-Specific Surface Embeddings for Articulated 3D Shape Reconstruction. NeurIPS 2021.</a> <br>
	    <a href="https://lasr-google.github.io/"> LASR: Learning Articulated Shape Reconstruction from a Monocular Video. CVPR 2021.</a> <br>
	    <a href="https://dove3d.github.io/"> DOVE: Learning Deformable 3D Objects by Watching Videos. arXiv preprint.</a> <br>
        Deformable shape reconstruction from images:<br>
        <a href="https://fkokkinos.github.io/to_the_point/"> To The Point: Correspondence-driven monocular 3D category reconstruction. NeurIPS 2021.</a> <br>
        <a href="https://sites.google.com/nvidia.com/unsup-mesh-2020/">Self-supervised Single-view 3D Reconstruction via Semantic Consistency. ECCV 2020.</a> <br>
        <a href="https://shubham-goel.github.io/ucmr/">Shape and Viewpoints without Keypoints. ECCV. 2020.</a> <br>
        <a href="https://nileshkulkarni.github.io/acsm/">Articulation Aware Canonical Surface Mapping. CVPR 2020.</a> <br>
        <a href="https://akanazawa.github.io/cmr/">Learning Category-Specific Mesh Reconstruction from Image Collections. ECCV 2018.</a> <br>
	    </p>
    </div> -->

<!--     <div class="content">
            <h2>Acknowledgments</h2>
            <p>Work is done during Meta AI internship. 
            Gengshan Yang is supported by the Qualcomm Innovation Fellowship. 
            Thanks to Shubham Tulsiani, Jason Zhang, and Ignacio Rocco for helpful feedback and discussions, 
            and Vasil Khalidov for help settting up DensePose-CSE color visualization.
            Thanks pet friends pikachiu, tetres, 
            <a href="https://www.youtube.com/c/HarutheShibaInu/about">haru </a>,
            <a href="https://www.instagram.com/cocososoxdxd/">coco and socks </a> 
            for serving as our models.
            </p>
    </div> -->


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
<tr><td>
    <p align="right"><font size="2">
        <a href="https://gengshan-y.github.io/">Webpage design borrowed from Gengshan Yang</a> </font>
    </p>
</td></tr>
</table>

</body></html> 
